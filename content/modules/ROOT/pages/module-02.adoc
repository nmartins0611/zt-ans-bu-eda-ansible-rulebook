= Getting Started: Kafka Example

:toc:
:toclevels: 3
:numbered:
:icons: font

== Overview

This tutorial demonstrates how to use `ansible-rulebook` with a Kafka source to monitor event streams and respond to specific messages. You'll learn how to integrate with existing Kafka infrastructure for event-driven automation.

== Introduction to Kafka Integration

Kafka allows us to have events in an event stream which we can subscribe to. This capability significantly expands our possibilities and opens the door to monitoring applications or enterprise environments using existing event streams.

In this example, we will:

* Subscribe to a Kafka topic stream
* Monitor for specific messages
* Execute automated responses when conditions are met

[NOTE]
====
Event streams provide a powerful foundation for enterprise-scale event-driven automation, allowing integration with existing messaging infrastructure.
====

== Kafka Topic Source Configuration

In this example, we will use Kafka messages from a topic and respond to specific message content.

=== Step 1: Examine the Kafka Rulebook

. **From the ansible-rulebook tab**, inspect the `kafka-example.yml` rulebook and take note of the source plugin configuration.

. **Observe the source configuration** pointing to:
   * Kafka host: `broker`
   * Topic: `eda-topic`

. **Review the rule condition** that looks for messages on the topic matching the text "Ansible is cool"

=== Step 2: Start the Rulebook Engine

. **From the ansible-rulebook tab**, start `ansible-rulebook` against the Kafka rulebook:
+
[source,bash]
----
ansible-rulebook --rulebook kafka-example.yml -i inventory.yml --print-events
----

Now that we have `ansible-rulebook` listening to the Kafka topic, let's proceed to generate test messages.

=== Step 3: Initialize the Kafka Producer

. **From the kafka tab**, we need to connect to our Kafka container and start the producer client which will allow you to send messages to the topic:

+
[source,bash]
----
 oc rsh broker
----

+
[source,bash]
----
kafka-console-producer --bootstrap-server broker:9092 --topic eda-topic
----

[IMPORTANT]
====
This starts an interactive terminal client. You'll see a `>` prompt where you can type your messages. Press Enter after each message to send it to the topic.
====

=== Step 4: Test Message Processing

==== Test 1: Non-Matching Message

. **From the kafka tab**, send a message using the client started in the previous step. The following message should **not** trigger a response from `ansible-rulebook`:
+
[source,json]
----
{"message":"Ansible is good"}
----

. **From the ansible-rulebook tab**, observe that the message has been read from the topic but no action was triggered due to the condition mismatch.

==== Test 2: Matching Message

. **From the kafka tab**, send a message using the running Kafka client that will match the conditions in the active rulebook:
+
[source,json]
----
{"message":"Ansible is cool"}
----

. **From the ansible-rulebook tab**, observe that:
   * The message has been read from the topic
   * The condition was satisfied
   * The playbook has been executed

=== Step 5: Working with Event Data

Event data is automatically passed to the Ansible playbook, enabling dynamic responses based on message content.

. **Test enhanced message processing** by sending another message that includes additional data:
+
[source,json]
----
{"message":"Ansible is cool", "sender":"YOUR_NAME"}
----
+
Replace `YOUR_NAME` with your actual name.

. **Observe the enhanced processing** in the ansible-rulebook output, noting how the additional `sender` field is available to the playbook.

[TIP]
====
In the `say-what.yml` playbook, the variable `ansible_eda.event` is specified, which allows us to reference all the event data passed from the Kafka message. This enables dynamic playbook execution based on message content.
====

== Understanding Event Data Flow

The event data flow in this Kafka integration works as follows:

1. **Message Reception**: Kafka messages are received by the ansible-rulebook source plugin
2. **Condition Evaluation**: The rulebook evaluates each message against defined conditions
3. **Event Data Passing**: When conditions are met, the entire event payload is passed to the target playbook
4. **Dynamic Execution**: Playbooks can access event data through the `ansible_eda.event` variable

== Key Concepts

**Event Streams**:: Kafka topics provide continuous streams of events that can be monitored in real-time

**Message Filtering**:: Rulebook conditions act as filters to determine which messages trigger actions

**Dynamic Playbooks**:: Event data enables playbooks to adapt their behavior based on message content

**Enterprise Integration**:: Kafka integration allows leveraging existing messaging infrastructure for automation

== Next Steps

* Explore more complex Kafka message filtering conditions
* Integrate with multiple Kafka topics using different source configurations
* Develop playbooks that process various event data fields
* Implement error handling and message acknowledgment patterns
* Scale to production Kafka clusters with authentication and SSL
